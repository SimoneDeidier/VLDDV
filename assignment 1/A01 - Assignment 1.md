# A01 - Assignment 1
* **Deidier Simone** - *student number: 133020*

## Theory questions

1. The **Flash Translation Layer (FTL)** is a fundamental component in **Solid State Drives (SSDs)** that manages the translation of logical addresses (those used by the operating system) into physical addresses (those actually present on the flash memory chips). The **FTL** maintains a *mapping table* that associates logical addresses with physical addresses. When the operating system requests data to be read or written, the **FTL** translates the logical address provided by the operating system into the corresponding physical address. Flash memory has a limited number of write/read cycles. The **FTL** implements *wear-leveling algorithms* to evenly distribute writes across all memory blocks, preventing some blocks from wearing out faster than others. When data is erased, it is not immediately removed from the flash memory. The **FTL** periodically performs *garbage collection operations* to reclaim free space by consolidating valid data and deleting unused blocks. In summary, the **FTL** is responsible for *optimising the performance, durability, and reliability* of **SSDs** by efficiently managing the underlying flash memory.

2. **SSDs** are designed to handle *sequential writes* better than *random writes*. Sequential writes allow large blocks of data to be written continuously, reducing the number of management operations required. Sequential writes reduce data fragmentation. Less fragmentation means that data is stored in contiguous blocks, facilitating successive read and write operations. With sequential writes, the **FTL** can handle *garbage collection* better, since the data is written in an orderly manner. This reduces the need to move data during clean-up operations, improving overall performance. Sequential writes also help with *wear leveling* by evenly distributing writes across all memory blocks. This helps prolong **SSD** life. Data written sequentially is easier to read efficiently. Sequential read operations can be performed faster than random reads.

3. When data blocks are correctly aligned to **SSD** pages, write operations can be performed more efficiently. Each write corresponds exactly to one or more pages, reducing the number of operations required. Correctly aligned data reduces fragmentation, as data blocks are stored contiguously. This facilitates subsequent read and write operations. With correct alignment, *garbage collection operations* are more efficient, as valid data can be consolidated more easily and unused blocks can be deleted without having to move a lot of data. Correct alignment helps with *wear leveling* by evenly distributing writes across all memory blocks. This helps prolong **SSD** life. The overall performance of the **SSD** improves with proper alignment, as read and write operations are faster and less prone to overhead.

4. **RocksDB** is a storage engine based on the **LSM (Log-Structured Merge) tree**, which uses two main data structures to manage data: **MemTable** and **SSTable**. The **MemTable** is a data structure in memory that stores recent writes. It is generally implemented as a skip list or a red-black tree to allow efficient insertions, updates and searches. When an application writes data, it is first inserted into the **MemTable**. Read operations search the **MemTable** first to find the most recent data. When the **MemTable** reaches a certain size, it is converted to an **SSTable** and written to disk. To ensure durability, each write operation is also recorded in a **Write-Ahead Log (WAL)** before being inserted into the **MemTable**. In the event of a crash, the **WAL** can be used to rebuild the **MemTable**. **SSTables** are immutable files stored on disk that contain a set of ordered keys and values. Each **SSTable** consists of several components: **Data Block**, **Index Block**, **Filter Block** and **Meta Block**. When the **MemTable** is downloaded to disk, it is converted into an **SSTable**. **SSTables** are immutable, which means that once written, they are never changed. Read operations first search the **MemTable** and then the **SSTables**, starting with the most recent **SSTable**.

5. **Compacting** in **RocksDB** is a crucial process for maintaining database performance and efficiency. **RocksDB** selects one or more **SSTables** to be compacted based on criteria such as size and level; the selected ones are locked to prevent changes during compaction. The selected **SSTables** are read into memory and the keys and values are sorted and merged, removing obsolete versions and deleted data. The sorted data are written into new **SSTables**; these new **SSTables** replace the old ones, which are then deleted. The **SSTable mapping table** is updated to reflect the new **SSTables**, the old **SSTables** are removed from the disk.

6. **LSM-trees** optimise *sequential writes*. Writes are first accumulated in a structure in memory (**MemTable**) and then written to disk in large, ordered blocks (**SSTable**). This reduces the number of random I/O operations, while **B+-trees** require random writes to maintain data order. Each insertion can cause nodes to split and changes to propagate upwards, resulting in many random I/O operations. **LSM-trees** use *compaction* to merge and sort data periodically. This process eliminates obsolete data and reduces fragmentation, improving the efficiency of successive reads and writes, **B+-trees** do not have a similar compaction mechanism. The structure can become fragmented over time, reducing the efficiency of read and write operations. *Sequential writes* of **LSM-trees** reduce disk wear, as data blocks are written contiguously. This is especially important for **SSDs**, which have a limited number of write cycles, while random writes of **B+-trees** can cause uneven disk wear, reducing the overall life of the device. **LSM-trees** are designed to optimise *write performance*, whereas the *write performance* of **B+-trees** can degrade with increasing data volume, due to frequent node splitting operations and the need to maintain data order. **LSM-trees** utilise memory efficiently by accumulating writes in a memory structure (**MemTable**) before writing them to disk. This reduces the number of I/O operations and improves overall performance, **B+-trees** require more memory to maintain the tree structure and indexes, especially when the data volume increases.

7. **Fault tolerance** is crucial to ensure the *reliability and availability* of large-scale distributed data systems.

    * **Hardware failures**: Hard disk drives (**HDDs**) and solid-state drives (**SSDs**) can fail due to wear and tear, manufacturing defects or physical damage. A server can fail due to power problems, overheating, or failure of internal components such as **CPU**, **RAM**, or motherboards. Network connectivity problems may disrupt communication between nodes in the distributed system.
    * **Software Errors**: Programming errors can cause unexpected behaviour, crashes or data corruption. Incorrect configurations can lead to system malfunctions or security vulnerabilities. Software updates may introduce new bugs or incompatibilities.
    * **Human errors**: Operators may make mistakes while managing the system, such as deleting data by mistake or misconfiguring components. Users may expose the system to security risks, such as using weak passwords or sharing credentials. Inadequate planning can lead to system overloads, lack of resources or unplanned downtime.

8. To achieve **fault tolerance** in large-scale distributed data systems, it is necessary to implement a number of techniques and practices covering various aspects of the system. The main techniques are:

    * Copying data across multiple nodes or data centres to ensure that a copy is always available in the event of a failure.
    * Splitting data into smaller parts (*shards*) and distributing them across several nodes to balance the load and improve scalability.
    * Distributing data requests evenly across nodes to avoid overloads and improve performance.
    * Automatically switching to a backup node in the event of a failure of a primary node.
    * Create back-up copies of data and metadata so that they can be restored in the event of loss or corruption.
    * Continuously monitor system status and record significant events to detect and diagnose problems.
    * Manage and distribute system configurations in a centralised and controlled manner.
    * Perform regular testing to verify the system's ability to withstand failures.

9. 